{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install libraries\n",
        "\n",
        "RAG Stage: Environment Setup\n",
        "We install all required libraries that will give us the ability to load documents, split them, convert them into embeddings (vectors), store them in a vector database, and run an LLM locally for Q/A."
      ],
      "metadata": {
        "id": "-jq1P1b9UGDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community chromadb sentence-transformers transformers accelerate\n"
      ],
      "metadata": {
        "id": "Y7b9fu6ZJoRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load website\n",
        "\n",
        "RAG Stage: Document Loading (Knowledge Source Collection)\n",
        "We load a public webpage from Python documentation. The loader extracts visible text from the website and stores it in a list of Document objects (docs), which will later be chunked and embedded."
      ],
      "metadata": {
        "id": "m621Ehs2UI_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(web_paths=[\"https://docs.python.org/3/tutorial/introduction.html#lists\"])\n",
        "docs = loader.load()\n",
        "\n",
        "print(\"Pages loaded:\", len(docs))\n",
        "print(\"\\nPreview:\\n\", docs[0].page_content[:300])\n"
      ],
      "metadata": {
        "id": "dxVRelDtKIix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into chunks\n",
        "\n",
        "RAG Stage: Chunking (Text Preprocessing for Retrieval)\n",
        "Large text is harder to embed and search. So we split the webpage text into smaller overlapping chunks (500 characters each, 50 characters overlap) so that each chunk contains meaningful, searchable information without losing continuity."
      ],
      "metadata": {
        "id": "LZlsvq8IURbq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn58rSirKd84"
      },
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "\n",
        "splits = splitter.split_documents(docs)\n",
        "\n",
        "print(\"Total chunks created:\", len(splits))\n",
        "print(\"\\nExample chunk:\\n\", splits[0].page_content[:200])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load embedding model\n",
        "\n",
        "RAG Stage: Embedding Creation (Convert text â†’ meaning vectors)\n",
        "We load a lightweight open embedding model from Sentence Transformers. This model will convert each chunk of text into a numerical vector that captures its semantic meaning. These vectors are used for similarity search in retrieval."
      ],
      "metadata": {
        "id": "5nRKKWi_UaBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "print(\"Embedding model loaded!\")\n"
      ],
      "metadata": {
        "id": "ty34k0pZLEoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store vectors in ChromaDB + Initialize Retriever\n",
        "\n",
        "RAG Stage: Indexing & Retrieval Setup\n",
        "We convert each text chunk into an embedding vector and store it in ChromaDB. We then initialize a retriever, which is a meaning-based search tool that will later fetch the most relevant chunks for any user question."
      ],
      "metadata": {
        "id": "UroeFtbqUfH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "vector_db = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding_model\n",
        ")\n",
        "\n",
        "retriever = vector_db.as_retriever()\n",
        "\n",
        "print(\"Vector database created!\")\n",
        "print(\"Total vectors stored:\", vector_db._collection.count())\n"
      ],
      "metadata": {
        "id": "t311Ll_sLGmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve context for a question\n",
        "\n",
        "RAG Stage: Retrieval (Search by meaning and fetch relevant context)\n",
        "We input a question. The retriever converts it into an embedding internally and finds the most semantically similar vectors from the DB. It returns the top matching chunks as Document objects, which we print to verify grounding context."
      ],
      "metadata": {
        "id": "ZXPNJ8BqUjth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are Python lists?\"\n",
        "\n",
        "retrieved_docs = retriever.invoke(question)\n",
        "\n",
        "print(\"Retrieved knowledge chunks:\\n\")\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(i+1, \"->\", doc.page_content[:150], \"\\n\")"
      ],
      "metadata": {
        "id": "2i6_5uo1LN2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the LLM\n",
        "\n",
        "RAG Stage: LLM Setup (Load free open Q/A-capable model)\n",
        "We now load a small open-source LLM trained for Q/A and prompt following. flan-t5-base is fully open (not gated), fast, and better than GPT-2 for instruction-based answers. It will generate answers from our augmented prompt."
      ],
      "metadata": {
        "id": "hNVT_ZMXUpVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "llm = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    max_new_tokens=150\n",
        ")\n",
        "\n",
        "print(\"LLM loaded!\")\n"
      ],
      "metadata": {
        "id": "Z_HmkeFuOwye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augment prompt and generate answer\n",
        "\n",
        "RAG Stage: Augmented Generation (Insert retrieved context into prompt and answer)\n",
        "We combine retrieved chunks into one text block, build a prompt containing only that context and the question, and pass it to the LLM. The LLM reads the real website knowledge and generates a grounded answer from it."
      ],
      "metadata": {
        "id": "FApGZH1-UyNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Answer the question using only the context below.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "print(\"Prompt sent to LLM:\\n\", prompt)\n",
        "\n",
        "response = llm(prompt)\n",
        "print(\"\\nFinal Answer:\\n\", response[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "id": "bihdJJACO368"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- RAG Stage: Retrieval --------\n",
        "question = \"Explain Python lists in simple words\"\n",
        "\n",
        "retrieved_docs = retriever.invoke(question)\n",
        "\n",
        "print(\"Retrieved Chunks:\\n\")\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(i+1, \"->\", doc.page_content[:150], \"\\n\")\n",
        "\n",
        "# -------- RAG Stage: Augmentation --------\n",
        "context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Answer the question using only the context below.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "print(\"Prompt sent to LLM:\\n\", prompt)\n",
        "\n",
        "# -------- RAG Stage: Generation --------\n",
        "response = llm(prompt)\n",
        "\n",
        "print(\"\\nFinal Answer:\\n\", response[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "id": "rRXPmhzqQxTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- RAG Stage: Retrieval --------\n",
        "question = \"How do Python loops work?\"\n",
        "\n",
        "retrieved_docs = retriever.invoke(question)\n",
        "\n",
        "print(\"Retrieved Chunks:\\n\")\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(i+1, \"->\", doc.page_content[:150], \"\\n\")\n",
        "\n",
        "# -------- RAG Stage: Augmentation --------\n",
        "context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Answer the question using only the context below.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "print(\"Prompt sent to LLM:\\n\", prompt)\n",
        "\n",
        "# -------- RAG Stage: Generation --------\n",
        "response = llm(prompt)\n",
        "\n",
        "print(\"\\nFinal Answer:\\n\", response[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "id": "0UX5wdzlQ6U4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}